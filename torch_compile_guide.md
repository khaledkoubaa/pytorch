# ğŸš€ Full Guide to `torch.compile` in PyTorch ğŸš€

`torch.compile` is a transformative feature introduced in PyTorch 2.0, designed to make your PyTorch code run faster with minimal code changes. This guide provides a comprehensive overview of `torch.compile`, from basic usage to advanced concepts and troubleshooting.

## ğŸ“š Table of Contents
1.  [Introduction to `torch.compile`](#introduction-to-torchcompile)
    *   [What is `torch.compile`? ğŸ¤”](#what-is-torchcompile)
    *   [Why use `torch.compile`? ğŸ’¡](#why-use-torchcompile)
2.  [Core Concepts ğŸ§ ](#core-concepts)
    *   [TorchDynamo (Graph Acquisition) ğŸ£](#torchdynamo-graph-acquisition)
    *   [AOTAutograd (Ahead-of-Time Backward Pass) âª](#aotautograd-ahead-of-time-backward-pass)
    *   [PrimTorch (Operator Set Canonicalization) ğŸ“](#primtorch-operator-set-canonicalization)
    *   [TorchInductor (Compiler Backend Infrastructure) ğŸ­](#torchinductor-compiler-backend-infrastructure)
3.  [Basic Usage ğŸ› ï¸](#basic-usage)
    *   [Compiling an `nn.Module` ğŸ§±](#compiling-an-nnmodule)
    *   [Compiling a Plain Function ğŸ“„](#compiling-a-plain-function)
    *   [Measuring Performance â±ï¸](#measuring-performance)
4.  [Compiler Backends âš™ï¸](#compiler-backends)
    *   [Overview of Backends ğŸ—ºï¸](#overview-of-backends)
    *   [`inductor` (Default) ğŸ†](#inductor)
    *   [`cudagraphs` ğŸ“ˆ](#cudagraphs)
    *   [Other Backends (e.g., `ipex`) ğŸ§©](#other-backends-eg-ipex)
    *   [How to Specify a Backend ğŸ·ï¸](#how-to-specify-a-backend)
    *   [No-Operation Backend (`noop`) ğŸš«](#no-operation-backend-noop)
    *   [Debugging Backends (`eager`, `aot_eager`) ğŸ](#debugging-backends-eager-aot_eager)
5.  [Compilation Modes âš™ï¸ğŸ”§](#compilation-modes)
    *   [`default`](#default)
    *   [`reduce-overhead`](#reduce-overhead)
    *   [`max-autotune`](#max-autotune)
    *   [`max-autotune-no-cudagraphs`](#max-autotune-no-cudagraphs)
    *   [Choosing the Right Mode âœ…](#choosing-the-right-mode)
6.  [Handling Dynamic Shapes ã€°ï¸](#handling-dynamic-shapes)
    *   [Explanation of `dynamic=True`](#explanation-of-dynamictrue)
    *   [Example with Dynamic Shapes](#example-with-dynamic-shapes)
    *   [Limitations and Considerations](#limitations-and-considerations-dynamic-shapes)
7.  [Graph Breaks ğŸ’”](#graph-breaks)
    *   [What are Graph Breaks?](#what-are-graph-breaks)
    *   [Why do Graph Breaks Happen?](#why-do-graph-breaks-happen)
    *   [How to Identify Graph Breaks ğŸ”](#how-to-identify-graph-breaks)
    *   [Impact on Performance ğŸ“‰](#impact-on-performance)
    *   [Minimizing Graph Breaks ğŸ©¹](#minimizing-graph-breaks)
8.  [Gotchas, Limitations, and Best Practices âš ï¸ğŸ“](#gotchas-limitations-and-best-practices)
    *   [Serialization of Compiled Models ğŸ’¾](#serialization-of-compiled-models)
    *   [Side Effects in Models ğŸ’¥](#side-effects-in-models)
    *   [Debugging Compiled Code ğŸ›](#debugging-compiled-code)
    *   [When `torch.compile` Might Not Be Beneficial ğŸš«ğŸ’¡](#when-torchcompile-might-not-be-beneficial)
    *   [CUDA Graphs Considerations ğŸ“Š](#cuda-graphs-considerations)
    *   [Stochasticity (Randomness) ğŸ²](#stochasticity-randomness)
9.  [Troubleshooting Common Issues ğŸ› ï¸ğŸ†˜](#troubleshooting-common-issues)
    *   [Compilation is Slow ğŸ¢](#compilation-is-slow)
    *   [Code Crashes with `torch.compile` ğŸ’¥ğŸ’»](#code-crashes-with-torchcompile)
    *   [Incorrect Results (Accuracy Issues) ğŸ“‰â“](#incorrect-results-accuracy-issues)
    *   [Out Of Memory (OOM) Errors ğŸ¤¯](#out-of-memory-oom-errors)
    *   [Excessive Recompilation ğŸ”„](#excessive-recompilation)
10. [Advanced Options âš™ï¸âœ¨](#advanced-options)
    *   [`fullgraph=True` ğŸ“ˆğŸ”—](#fullgraphtrue)
    *   [Backend-specific `options` ğŸ”§](#backend-specific-options)
    *   [Disabling Compilation for Specific Functions (`@torch.compiler.disable`) ğŸš«function](#disabling-compilation-for-specific-functions-torchcompilerdisable)
11. [Conclusion and Further Resources ğŸğŸ“š](#conclusion-and-further-resources)

---

## 1. Introduction to `torch.compile` ğŸŒŸ

### What is `torch.compile`? ğŸ¤”
`torch.compile` is a Python function decorator and a standalone function that JIT (Just-In-Time) compiles your PyTorch models or functions into optimized kernels. It aims to provide significant speedups by reducing Python overhead, fusing operations, and leveraging hardware-specific optimizations, all while requiring minimal to no changes to your existing eager-mode PyTorch code.

It's designed to be:
*   âš¡ **Performant:** Offering substantial speedups for many models.
*   ğŸ **Pythonic:** Integrating seamlessly with Python and PyTorch's eager mode.
*   ğŸ¤¸ **Flexible:** Supporting dynamic shapes and a wide range of PyTorch features.

### Why use `torch.compile`? ğŸ’¡
The primary motivation behind `torch.compile` is to bridge the gap between ease-of-use and performance.
*   ğŸš€ **Speed:** Automatically optimize your models for faster training and inference.
*   ğŸ¤ **Ease of Use:** Apply with a single line of code (`@torch.compile` or `compiled_model = torch.compile(model)`).
*   ğŸ”’ **Backward Compatibility:** 100% backward compatible with PyTorch eager mode. If a part of the code cannot be compiled, it automatically falls back to eager execution for that part (this is called a "graph break").
*   ğŸ’¨ **Reduced Overhead:** Especially beneficial for models where Python overhead is a bottleneck.

---

## 2. Core Concepts ğŸ§ 
`torch.compile` is built upon a stack of innovative technologies:

```plaintext
Your PyTorch Code (Python)
â”‚
â”œâ”€> TorchDynamo (Graph Acquisition) ğŸ£
â”‚   â”‚   Safely captures Python bytecode into FX Graphs.
â”‚   â”‚   Handles Python's dynamism, inserts guards.
â”‚   â””â”€> Graph Breaks (if parts are unacquirable, fallback to eager) ğŸ’”
â”‚
â”œâ”€> AOTAutograd (Ahead-of-Time Backward Pass) âª
â”‚   â”‚   Traces autograd engine to generate forward/backward graphs.
â”‚   â””â”€> FX Graph (representing forward & backward)
â”‚
â”œâ”€> PrimTorch (Operator Set Canonicalization) ğŸ“ (Optional Step)
â”‚   â”‚   Decomposes PyTorch ops into a smaller set of primitive ops.
â”‚   â””â”€> FX Graph (with primitive ops)
â”‚
â””â”€> Compiler Backend (e.g., TorchInductor) ğŸ­
    â”‚   Takes FX Graph (potentially with prim ops).
    â”œâ”€> TorchInductor ğŸ†
    â”‚   â”‚   Generates optimized code (Triton for GPU, C++ for CPU).
    â”‚   â”‚   Performs fusion, memory optimization, etc.
    â”‚   â””â”€> Optimized Kernels (Triton/C++) âœ¨
    â”‚
    â””â”€> Other Backends (e.g., NNC, TVM, ONNXRT) ğŸ§©
        â”‚   Each has its own compilation strategy.
        â””â”€> Backend-Specific Optimized Code
```

### TorchDynamo (Graph Acquisition) ğŸ£
TorchDynamo is the component responsible for safely capturing PyTorch programs from their Python bytecode into an FX Graph. It uses a technique based on Python's Frame Evaluation API ([PEP 523](https://peps.python.org/pep-0523/)).
*   âœ… **Reliability:** It can capture a very high percentage of PyTorch programs without requiring code modification.
*   ğŸ›¡ï¸ **Safety:** It inserts "guards" to check if the assumptions made during graph capture still hold true for subsequent runs. If a guard fails, the code is recompiled, ensuring correctness.
*   ğŸ’” **Graph Breaks:** When TorchDynamo encounters Python features it cannot safely trace (e.g., complex control flow dependent on tensor values, some third-party C extensions), it performs a "graph break." This means the current graph is compiled and executed, then Python execution resumes for the unsupported part, and Dynamo attempts to capture a new graph afterwards.

### AOTAutograd (Ahead-of-Time Backward Pass) âª
For training, `torch.compile` needs to optimize both the forward and backward passes. AOTAutograd (Ahead-Of-Time Autograd) is responsible for this.
*   It traces PyTorch's autograd engine to generate an FX graph for the backward pass *before* actual execution.
*   This allows the compiler backend (like TorchInductor) to optimize the forward and backward computations jointly or separately.
*   It helps in minimizing the amount of state that needs to be saved between the forward and backward passes.

### PrimTorch (Operator Set Canonicalization) ğŸ“
PyTorch has a large number of operators (~2000+ including overloads). Writing a compiler backend that supports all of them is a monumental task. PrimTorch aims to simplify this by defining a smaller, more stable set of primitive operators (~250 "prim ops").
*   Most PyTorch operators can be decomposed into these primitive operators.
*   This significantly lowers the barrier for hardware vendors and compiler developers to write backends for PyTorch, as they only need to target this smaller, more manageable operator set.

### TorchInductor (Compiler Backend Infrastructure) ğŸ­
TorchInductor is the default compiler backend for `torch.compile`. It takes the FX graphs (from TorchDynamo and AOTAutograd, potentially decomposed into PrimTorch ops) and generates fast, executable code.
*   ğŸ **Define-by-Run IR:** It uses a Pythonic, define-by-run loop-level IR, making it more hackable and extensible.
*   ğŸ’» **GPU Code Generation:** For NVIDIA and AMD GPUs, TorchInductor primarily leverages [OpenAI Triton](https://openai.com/research/triton) to generate high-performance CUDA or HIP kernels. Triton is a language and compiler for writing highly efficient custom deep learning primitives.
*   ğŸ§  **CPU Code Generation:** For CPUs, TorchInductor generates C++ code using OpenMP for parallelism and leverages standard CPU vectorization capabilities. It can also offload operations to libraries like MKLDNN (oneDNN) where beneficial.
*   âœ¨ **Key Optimizations:** TorchInductor performs various optimizations like operator fusion (vertical and horizontal), memory planning, and choosing optimal kernel schedules.

---

## 3. Basic Usage ğŸ› ï¸

### Compiling an `nn.Module` ğŸ§±
The most common use case is compiling a `torch.nn.Module`.

```python
import torch
import torchvision.models as models
import time

# Ensure you have a CUDA-enabled GPU for best results, or it will run on CPU.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device} ğŸ’»")

# 1. Define or load your model
model = models.resnet18().to(device)

# 2. Compile the model âš™ï¸
# You can call .compile() directly on the module instance (available from PyTorch 2.1+)
# This is generally preferred over torch.compile(model) to avoid state_dict key issues.
if hasattr(model, 'compile'):
    print("Using model.compile()")
    model.compile() # Default backend is 'inductor'
else:
    # For older PyTorch versions or if model.compile() is not available
    print("Using torch.compile(model)")
    model = torch.compile(model)


# 3. Use the compiled model as usual âœ…
dummy_input = torch.randn(16, 3, 224, 224, device=device)

# First run will include compilation overhead â³
print("Running compiled model (first run, includes compilation)...")
start_time = time.perf_counter()
output_compiled_first = model(dummy_input)
if device.type == 'cuda':
    torch.cuda.synchronize()
end_time = time.perf_counter()
print(f"First run (compiled) took: {end_time - start_time:.4f} seconds")

# Subsequent runs should be faster ğŸš€
print("\nRunning compiled model (subsequent run)...")
start_time = time.perf_counter()
output_compiled_second = model(dummy_input)
if device.type == 'cuda':
    torch.cuda.synchronize()
end_time = time.perf_counter()
print(f"Second run (compiled) took: {end_time - start_time:.4f} seconds")

# For comparison, run the original model (if you didn't use model.compile() in-place)
# original_model = models.resnet18().to(device)
# print("\nRunning original eager model...")
# start_time = time.perf_counter()
# output_eager = original_model(dummy_input)
# if device.type == 'cuda':
# torch.cuda.synchronize()
# end_time = time.perf_counter()
# print(f"Eager run took: {end_time - start_time:.4f} seconds")
```

### Compiling a Plain Function ğŸ“„
You can also compile standalone Python functions that use PyTorch operations.

```python
import torch

def my_custom_function(x, y):
    a = torch.sin(x)
    b = torch.cos(y)
    return a + b

# Compile the function
compiled_function = torch.compile(my_custom_function)

# Use it
tensor1 = torch.randn(5, 5, device=device) # Assuming 'device' is defined from previous cell
tensor2 = torch.randn(5, 5, device=device)

result = compiled_function(tensor1, tensor2)
print("\nOutput from compiled plain function:")
print(result)
```

### Measuring Performance â±ï¸
When measuring performance, keep these in mind:
1.  â³ **Compilation Overhead:** The first call to a compiled function/model will be slower because it includes the time taken to compile the code. Subsequent calls with compatible inputs will use the cached compiled code and should be faster.
2.  ğŸ”¥ **Warm-up:** Especially on GPUs, perform a few warm-up runs before starting measurements to ensure stable performance numbers (e.g., for CUDA kernel loading, clock stabilization).
3.  â›“ï¸ **GPU Synchronization:** If measuring GPU code, always use `torch.cuda.synchronize()` before starting and after ending timing to ensure all GPU operations have completed.
4.  ğŸ“Š **Averaging:** Run the operation multiple times and average the results (or take the median) to get a more reliable performance metric.
5.  ğŸ”„ **Input Variation:** If your input shapes or other guarded properties change frequently, you might experience recompilation, which impacts performance.

```python
import torch
import time
import numpy as np

# Example model for benchmarking
class SimpleNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = torch.nn.Linear(1024, 512)
        self.relu = torch.nn.ReLU()
        self.fc2 = torch.nn.Linear(512, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\nBenchmarking on device: {device}")

def benchmark_model(model, input_tensor, num_runs=100, desc="Model"):
    times = []
    # Warm-up runs
    for _ in range(10):
        _ = model(input_tensor)
    if device.type == 'cuda':
        torch.cuda.synchronize()

    for _ in range(num_runs):
        start_time = time.perf_counter()
        _ = model(input_tensor)
        if device.type == 'cuda':
            torch.cuda.synchronize()
        end_time = time.perf_counter()
        times.append(end_time - start_time)

    median_time = np.median(times)
    print(f"{desc} - Median time over {num_runs} runs: {median_time:.6f} seconds")
    return median_time

# Create model instances
eager_model = SimpleNet().to(device)
# For a fair comparison, compile first then benchmark
compiled_model = torch.compile(SimpleNet().to(device))

# Dummy input
batch_size = 256
dummy_input = torch.randn(batch_size, 1024, device=device)

# Benchmark eager model (after its own warm-up)
print("Benchmarking Eager Model...")
eager_median_time = benchmark_model(eager_model, dummy_input, desc="Eager Model")

# Benchmark compiled model (first call to compile includes overhead, so we do it outside benchmark)
# The benchmark_model function itself includes warm-up for the already-compiled model.
print("\nBenchmarking Compiled Model...")
# First call to ensure compilation happens if not already
_ = compiled_model(dummy_input)
if device.type == 'cuda':
    torch.cuda.synchronize()

compiled_median_time = benchmark_model(compiled_model, dummy_input, desc="Compiled Model")

if compiled_median_time > 0 and eager_median_time > 0 :
    speedup = eager_median_time / compiled_median_time
    print(f"\nSpeedup (Compiled vs Eager): {speedup:.2f}x")
else:
    print("\nCould not calculate speedup due to zero median times (likely very fast execution or issue).")

```

### Example Project Structure ğŸ“‚
A typical project utilizing `torch.compile` might look like this:

```plaintext
my_pytorch_project/
â”œâ”€â”€ main.py             # Main script for training/inference
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ custom_model.py   # Your nn.Module definitions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py        # Custom Dataset and DataLoader logic
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ helpers.py        # Utility functions
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml       # Configuration files
â””â”€â”€ requirements.txt      # Project dependencies
```
In `main.py` or within your training loop, you would apply `torch.compile` to your model instance.

---
## 4. Compiler Backends âš™ï¸

`torch.compile` uses a backend system to perform the actual compilation and code generation. You can specify which backend to use via the `backend` argument.
3.  â›“ï¸ **GPU Synchronization:** If measuring GPU code, always use `torch.cuda.synchronize()` before starting and after ending timing to ensure all GPU operations have completed.
4.  ğŸ“Š **Averaging:** Run the operation multiple times and average the results (or take the median) to get a more reliable performance metric.
5.  ğŸ”„ **Input Variation:** If your input shapes or other guarded properties change frequently, you might experience recompilation, which impacts performance.

```python
import torch
import time
import numpy as np

# Example model for benchmarking
class SimpleNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = torch.nn.Linear(1024, 512)
        self.relu = torch.nn.ReLU()
        self.fc2 = torch.nn.Linear(512, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\nBenchmarking on device: {device}")

def benchmark_model(model, input_tensor, num_runs=100, desc="Model"):
    times = []
    # Warm-up runs
    for _ in range(10):
        _ = model(input_tensor)
    if device.type == 'cuda':
        torch.cuda.synchronize()

    for _ in range(num_runs):
        start_time = time.perf_counter()
        _ = model(input_tensor)
        if device.type == 'cuda':
            torch.cuda.synchronize()
        end_time = time.perf_counter()
        times.append(end_time - start_time)

    median_time = np.median(times)
    print(f"{desc} - Median time over {num_runs} runs: {median_time:.6f} seconds")
    return median_time

# Create model instances
eager_model = SimpleNet().to(device)
# For a fair comparison, compile first then benchmark
compiled_model = torch.compile(SimpleNet().to(device))

# Dummy input
batch_size = 256
dummy_input = torch.randn(batch_size, 1024, device=device)

# Benchmark eager model (after its own warm-up)
print("Benchmarking Eager Model...")
eager_median_time = benchmark_model(eager_model, dummy_input, desc="Eager Model")

# Benchmark compiled model (first call to compile includes overhead, so we do it outside benchmark)
# The benchmark_model function itself includes warm-up for the already-compiled model.
print("\nBenchmarking Compiled Model...")
# First call to ensure compilation happens if not already
_ = compiled_model(dummy_input)
if device.type == 'cuda':
    torch.cuda.synchronize()

compiled_median_time = benchmark_model(compiled_model, dummy_input, desc="Compiled Model")

if compiled_median_time > 0 and eager_median_time > 0 :
    speedup = eager_median_time / compiled_median_time
    print(f"\nSpeedup (Compiled vs Eager): {speedup:.2f}x")
else:
    print("\nCould not calculate speedup due to zero median times (likely very fast execution or issue).")

```

---
## 4. Compiler Backends âš™ï¸

`torch.compile` uses a backend system to perform the actual compilation and code generation. You can specify which backend to use via the `backend` argument.

### Overview of Backends ğŸ—ºï¸
*   ğŸ£ **TorchDynamo** captures the graph.
*   âª **AOTAutograd** processes the graph to handle autograd.
*   ğŸ­ The resulting graph is then passed to a **compiler backend**.

### `inductor` (Default) ğŸ†
This is the **default backend** and the most actively developed.
*   ğŸ’» **GPU Support:** Uses OpenAI Triton to generate efficient CUDA kernels for NVIDIA GPUs (Volta, Ampere, Hopper and newer recommended) and HIP kernels for AMD GPUs.
*   ğŸ§  **CPU Support:** Generates optimized C++/OpenMP code.
*   âœ¨ **Features:** Performs extensive fusion (pointwise, reductions, etc.), memory optimization, and parallelization.

```python
compiled_model_inductor = torch.compile(model, backend="inductor") # Assuming model is defined
```

### `cudagraphs` ğŸ“ˆ
This backend leverages [CUDA Graphs](https://developer.nvidia.com/blog/cuda-graphs/) to reduce CPU overhead for GPU execution.
*   **How it works:** CUDA Graphs allow launching multiple GPU operations via a single CPU call, significantly reducing launch latency.
*   **Best for:** Models that are CPU-bound due to kernel launch overhead, often smaller models or models with many small CUDA operations.
*   **Limitations:** Less flexible than Inductor, especially with dynamic control flow or operations that are not CUDA graph compatible. `torch.compile` with the `cudagraphs` backend will attempt to segment the model into CUDA graph-compatible sections.

```python
# model should be on CUDA device
# compiled_model_cudagraphs = torch.compile(model.cuda(), backend="cudagraphs")
# Note: Often used via modes like "reduce-overhead" which may use cudagraphs internally.
# Direct use of "cudagraphs" backend is less common than "inductor".
# The "reduce-overhead" mode often implies cudagraphs usage where beneficial.
```
The `cudagraphs` backend is often implicitly used by `TorchInductor` when `torch._inductor.config.triton.cudagraphs = True` (which is the default). The `mode="reduce-overhead"` also heavily relies on CUDA graphs.

### Other Backends (e.g., `ipex`) ğŸ§©
Other backends exist, often for specific hardware or use cases:
*   `ipex`: Intel(R) Extension for PyTorch*, provides optimizations for Intel CPUs and GPUs.
*   `onnxrt`: (Experimental) Compiles using ONNX Runtime.
*   `tvm`: (Experimental) Apache TVM backend.

You can list available backends:
```python
import torch._dynamo.list_backends
print(f"Available backends: {torch._dynamo.list_backends.list_backends()}")
```

### How to Specify a Backend ğŸ·ï¸
Pass the backend name as a string to `torch.compile`:
```python
# Assuming SimpleNet and device are defined as in previous examples
# model_to_compile = SimpleNet().to(device)
# compiled_with_specific_backend = torch.compile(model_to_compile, backend="inductor") # Default
# if device.type == 'cpu':
    # For Intel CPUs, you might have 'ipex' available if installed.
    # try:
    # compiled_with_ipex = torch.compile(model_to_compile, backend="ipex")
    # print("Successfully compiled with IPEX backend.")
    # except RuntimeError as e:
    # print(f"Could not compile with IPEX (is it installed and configured?): {e}")
```

### No-Operation Backend (`noop`) ğŸš«
The `noop` backend performs graph acquisition with TorchDynamo but doesn't actually compile or change how the code executes. It runs the graph eagerly.
*   **Use Case:** Useful for debugging TorchDynamo itself or understanding the graph structure TorchDynamo captures without any backend interference.

```python
# compiled_noop = torch.compile(model, backend="noop") # Assuming model is defined
# output = compiled_noop(dummy_input) # Will run eagerly after graph capture
```

### Debugging Backends (`eager`, `aot_eager`) ğŸ
These are primarily for debugging the `torch.compile` stack itself:
*   `backend="eager"`: Runs TorchDynamo for graph capture, then executes the captured graph using PyTorch's eager mode. Helps isolate issues to TorchDynamo.
*   `backend="aot_eager"`: Runs TorchDynamo and AOTAutograd (to generate forward/backward graphs), then executes these graphs eagerly. Helps isolate issues to AOTAutograd.

```python
# For debugging purposes:
# compiled_debug_eager = torch.compile(model, backend="eager")
# compiled_debug_aot_eager = torch.compile(model, backend="aot_eager")
```

---

## 5. Compilation Modes âš™ï¸ğŸ”§
The `mode` argument in `torch.compile` allows you to specify optimization priorities.

### `default` (or `None`)
*   This is the standard mode.
*   Aims for a good balance between compilation time, runtime speedup, and memory usage.
*   ğŸ‘ Good starting point for most models.

```python
# compiled_default = torch.compile(model, mode="default") # Assuming model is defined
# or simply
# compiled_default = torch.compile(model)
```

### `reduce-overhead`
*   Focuses on minimizing framework overhead.
*   Particularly effective for smaller models or models with many small operations where Python overhead or kernel launch latency is the bottleneck.
*   Often uses techniques like CUDA Graphs more aggressively.
*   May slightly increase memory usage or compilation time compared to `default`.

```python
# compiled_reduce_overhead = torch.compile(model, mode="reduce-overhead")
```

### `max-autotune`
*   Instructs the backend (primarily TorchInductor with Triton) to spend more time searching for the absolute fastest kernel configurations.
*   â±ï¸ Compilation time can be significantly longer.
*   Best suited for models where training or inference will run for a very long time, making the upfront compilation cost worthwhile.
*   It explores a wider range of tiling sizes, kernel schedules, etc.

```python
# This can take a very long time to compile â³â³â³
# compiled_max_autotune = torch.compile(model, mode="max-autotune")
```

### `max-autotune-no-cudagraphs`
Similar to `max-autotune` but disables the usage of CUDA graphs. This can be useful if CUDA graphs are causing issues or instability for a particular model, while still wanting extensive kernel tuning.

```python
# compiled_max_autotune_no_cg = torch.compile(model, mode="max-autotune-no-cudagraphs")
```

### Choosing the Right Mode âœ…
*   Start with `default`. It provides good speedups for a wide range of models.
*   If your model is small or you suspect high framework overhead, try `reduce-overhead`.
*   If you need the absolute best performance for a long-running job and can afford very long compilation times, experiment with `max-autotune`.
*   ğŸ“Š Benchmark different modes on your specific model and hardware to find the optimal setting.

---

## 6. Handling Dynamic Shapes ã€°ï¸
`torch.compile` supports "dynamic shapes," meaning it can generate code that works for inputs of varying sizes without recompiling for each new shape.

### Explanation of `dynamic=True`
By default (`dynamic=None`), `torch.compile` might initially assume static shapes. If a recompilation is triggered due to a shape mismatch, it may then try to compile with dynamic shapes.
Setting `dynamic=True` explicitly tells `torch.compile` to anticipate and handle dynamic dimensions in tensors from the outset.
*   ğŸ§© **Symbolic Shapes:** TorchDynamo traces tensor dimensions symbolically (e.g., `s0`, `s1`) rather than as fixed integers.
*   ğŸ›¡ï¸ **Guards on Ranges:** Guards might be generated based on ranges or properties of these symbolic dimensions.
*   âš–ï¸ **Performance Trade-offs:** Compiling for dynamic shapes can sometimes prevent certain static-shape-specific optimizations, potentially leading to slightly slower code than a version compiled for a fixed static shape. However, it avoids costly recompilations if shapes vary.

### Example with Dynamic Shapes

```python
import torch

@torch.compile(dynamic=True)
def dynamic_model(x):
    return x.cos() * 2

# Run with different input shapes
# Assuming 'device' is defined from previous examples
input_tensor_1 = torch.randn(4, 128, device=device)
input_tensor_2 = torch.randn(8, 256, device=device) # Different shape

print("\nRunning model compiled with dynamic=True:")
# First call (compiles for dynamic shapes)
print(f"Input 1 shape: {input_tensor_1.shape}")
_ = dynamic_model(input_tensor_1)
print("Successfully ran with input 1")

# Second call with different shape (should not recompile if dynamic handling is successful)
print(f"Input 2 shape: {input_tensor_2.shape}")
_ = dynamic_model(input_tensor_2)
print("Successfully ran with input 2 (should use cached dynamic kernel)")

# To verify recompilations, you can use logging:
# import torch._dynamo
# torch._dynamo.reset() # Clear cache before new test
# torch._logging.set_logs(recompiles=True) # Requires PyTorch 2.1+ for torch._logging
# compiled_fn = torch.compile(lambda x: x+1, dynamic=True)
# compiled_fn(torch.randn(3, device=device))
# compiled_fn(torch.randn(4, device=device)) # Should ideally not show a recompile message
# torch._logging.set_logs(recompiles=False)
```
(Note: Actual recompile behavior can depend on the complexity of the model and the backend's capabilities for dynamic shapes.)

### Limitations and Considerations (Dynamic Shapes) âš ï¸
*   ğŸ¢ **Performance:** As mentioned, fully dynamic code might be slightly slower than code specialized for static shapes.
*   ğŸ’” **Graph Breaks:** Certain operations might still cause graph breaks even with `dynamic=True` if they are inherently difficult to make shape-agnostic.
*   âš™ï¸ **Backend Support:** The extent of dynamic shape support can vary between backends. TorchInductor has made significant progress.
*   ğŸ› **Complexity:** Debugging performance or issues with dynamic shapes can be more complex.
*   BOUNDED **Bounded Dynamism:** Often, dynamism is bounded (e.g., batch size varies but other dimensions are fixed). `torch.compile` tries to handle this. For fully symbolic (unbounded) dynamism, some ops might still be challenging.

---

## 7. Graph Breaks ğŸ’”

### What are Graph Breaks?
A graph break occurs when TorchDynamo is tracing a model's code and encounters a Python feature or operation it cannot convert into its FX graph representation. When this happens:
1.  The graph captured *so far* is compiled and executed.
2.  Python's eager interpreter takes over to execute the unsupported code.
3.  TorchDynamo then attempts to resume tracing and capture a *new* graph for the subsequent operations.

This results in the PyTorch program being executed as a sequence of compiled graph segments interspersed with eagerly executed Python code.

### Why do Graph Breaks Happen?
Common reasons for graph breaks include:
*   ğŸ **Data-dependent control flow:** `if` statements or loops where the condition depends on the actual values within a tensor (e.g., `if x.sum() > 0:`).
*   Unsupported Python built-ins or modules: Calls to certain Python built-in functions (e.g., `print()`, `isinstance()` on complex types) or functions from standard library modules (e.g., `inspect`, direct file I/O) that Dynamo cannot model.
*   EXT **Third-party C extensions:** Calling custom C/C++ extensions that are not PyTorch operations.
*   âœ‹ **Tensor data access:** Directly accessing tensor data in a way that Dynamo can't track symbolically (e.g., `.item()`, `.tolist()`, iterating over tensor elements directly in Python).
*   â“ **Unsupported PyTorch operations:** Although rare for core ops, some highly dynamic or obscure operations might not be fully traceable.
*   ğŸ”„ **Mutable data structures:** Complex manipulations of Python lists or dictionaries containing tensors in ways that are hard to track.

### How to Identify Graph Breaks ğŸ”
You can identify graph breaks using PyTorch's logging capabilities or `torch._dynamo.explain()`:

**Using `TORCH_LOGS` environment variable:**
```bash
# In your terminal
# TORCH_LOGS="graph_breaks" python your_script.py
```

**Programmatically:**
```python
import torch
import torch._dynamo
import logging # For older PyTorch versions, this might be torch._logging

# For PyTorch 2.1+
if hasattr(torch, '_logging'):
    torch._logging.set_logs(graph_breaks=True)
else: # For older versions, try this (may vary)
    logging.getLogger("torch._dynamo").setLevel(logging.DEBUG)
    # Or set specific options if available in torch._dynamo.config
    # torch._dynamo.config.log_level = logging.DEBUG # Example, exact config may differ

# Example function that will cause a graph break
def model_with_print(x):
    x = x * 2
    print(f"Intermediate tensor sum: {x.sum()}") # print() causes a graph break
    return x.cos()

# compiled_model_with_print = torch.compile(model_with_print)
# compiled_model_with_print(torch.randn(3,3, device=device)) # Assuming device is defined

# After running, reset logging if needed
if hasattr(torch, '_logging'):
    torch._logging.set_logs(graph_breaks=False)
```
The logs will typically show the reason for the graph break and the user code location.

**Using `torch._dynamo.explain()`:**
This utility runs TorchDynamo on your function/module and provides a report on graph breaks, number of graphs generated, and other diagnostics.

```python
import torch
import torch._dynamo

def model_with_data_dependent_if(x, y):
    res = x + y
    if res.mean() > 0: # Data-dependent control flow
        return res * 2
    else:
        return res / 2

# explain_output = torch._dynamo.explain(model_with_data_dependent_if)(torch.randn(5, device=device), torch.randn(5, device=device)) # Assuming device
# print("\n--- Explain Output ---")
# print(explain_output)
# print("--------------------")
```
The `explain` output will list "Break Reasons".

### Impact on Performance ğŸ“‰
Graph breaks can significantly hinder the performance benefits of `torch.compile`:
*   âœ‚ï¸ **Lost Fusion Opportunities:** Each graph segment is compiled independently. Operations cannot be fused across graph breaks.
*   ğŸŒ **Overhead:** Switching between compiled execution and eager execution incurs overhead.
*   â³ **Increased Compilation:** More graph segments mean more individual compilation units, potentially increasing overall compilation time.

Frequent graph breaks, especially within tight loops or performance-critical sections of your model, can negate any speedups from compilation.

### Minimizing Graph Breaks ğŸ©¹
*   **Avoid data-dependent control flow:** If possible, refactor code to use tensor operations that achieve similar logic (e.g., `torch.where`, masking) or lift conditions out of the compiled region if they can be determined from static inputs. `torch.cond` can sometimes be used for simple conditional logic.
*   **Limit Python built-ins:** Minimize calls to `print()`, `isinstance()`, and other problematic built-ins inside the core model logic. Move logging or debugging prints outside the compiled function or use them sparingly.
*   **Handle external calls:** If calling non-PyTorch libraries or C extensions, consider if these parts truly need to be inside the compiled region. If not, try to structure code so they are outside. If they must be inside, they will cause graph breaks.
*   **Refactor tensor data access:** Avoid `.item()`, `.numpy()`, or Python loops over tensor elements within performance-critical compiled code.
*   **Use `torch.compiler.disable`:** For sections of code that are inherently difficult to compile or not performance-critical but cause many graph breaks (like complex data preprocessing or postprocessing steps that are part of the `forward`), you can decorate them with `@torch.compiler.disable` to tell Dynamo to skip compiling them.

---

## 8. Gotchas, Limitations, and Best Practices âš ï¸ğŸ“

### Serialization of Compiled Models ğŸ’¾
*   ğŸ”‘ **State Dict:** You should always save and load the `state_dict` of the *original* `nn.Module`, not the module returned by `torch.compile` (unless you used `model.compile()` in-place, in which case they are the same object). The compiled model shares parameters with the original model.
    ```python
    # class MyModel(torch.nn.Module): ... # Define your model
    # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # original_model = MyModel().to(device)
    # compiled_model = torch.compile(original_model)
    # # ... training ...
    # torch.save(original_model.state_dict(), "my_model_state.pt")

    # # To load:
    # new_model_instance = MyModel().to(device)
    # new_model_instance.load_state_dict(torch.load("my_model_state.pt"))
    # # Optionally re-compile after loading
    # # new_compiled_model = torch.compile(new_model_instance)
    ```
*   ğŸš« **Saving the whole compiled model:** `torch.save(compiled_model, ...)` is generally **not recommended and may not work reliably**. The compiled artifacts are often specific to the environment and hardware. The recommended practice is to save the original model's `state_dict` and re-compile when deploying or resuming.
*   â¡ï¸ **Exporting for Inference (`torch.export`):** For deploying models to environments without Python or for more stable inference artifacts, use `torch.export`. `torch.export` aims to produce a more portable representation but has stricter requirements (e.g., typically `fullgraph=True` is needed for `torch.compile` before exporting).

### Side Effects in Models ğŸ’¥
`torch.compile` generally assumes functions are relatively pure.
*   â¡ï¸ **In-place modifications to inputs:** Modifying input tensors in-place within a compiled function can lead to unexpected behavior or errors, as the compiler might make assumptions that are violated. It's generally safer to return new tensors.
*   ğŸŒ **Modifying global state or non-local variables:** While TorchDynamo tries to handle some cases with guards, extensive modification of external state from within a compiled function can be problematic and reduce optimization opportunities or lead to correctness issues.
*   ğŸ”¢ **Order of operations:** The compiler might reorder operations for optimization. If your code relies on a specific order of side effects (e.g., printing, in-place updates to shared buffers), this reordering could change behavior.

### Debugging Compiled Code ğŸ›
Debugging code that runs through `torch.compile` can be challenging because the execution path is altered.
*   ğŸªµ **Use `TORCH_LOGS` and `torch._dynamo.explain()`:** As covered in the "Graph Breaks" section, these are invaluable for understanding what Dynamo is doing.
*   ğŸ”¬ **Simplify the problem:** Try to create a minimal reproducible example.
*   ğŸ”Œ **Disable `torch.compile`:** Temporarily remove `@torch.compile` or comment out the `torch.compile()` call to see if the issue persists in eager mode. This helps isolate whether the bug is in your original logic or related to compilation.
*   ğŸ•µï¸ **Use debugging backends:** `backend="eager"` or `backend="aot_eager"` can help pinpoint if the issue lies in TorchDynamo, AOTAutograd, or the final compiler backend (like Inductor).
*   ğŸš« **`torch.compiler.disable`:** Selectively disable compilation on parts of your code to narrow down where an issue might be occurring.
*   ğŸ”¬ **Minifier:** For crashes within the compiler stack, PyTorch provides a minifier tool (`TORCHDYNAMO_REPRO_AFTER="dynamo"` or `"aot"`) that attempts to automatically reduce your code to a minimal failing example. See the troubleshooting guide for details.

### When `torch.compile` Might Not Be Beneficial ğŸš«ğŸ’¡
*   ğŸœ **Very small models/ops:** If the computation is trivial, the overhead of compilation itself might outweigh any runtime gains.
*   ğŸ“‰ **Highly dynamic code with many graph breaks:** If your code has fundamental reasons for frequent graph breaks that cannot be easily refactored, `torch.compile` might offer little to no speedup or even slow things down.
*   ğŸ’¿ **CPU-bound I/O or preprocessing:** If your model's bottleneck is data loading or CPU-intensive preprocessing that isn't part of the compiled graph, `torch.compile` won't help that part.
*   â±ï¸ **Initial call latency is critical:** If the very first inference call must be extremely fast, the compilation overhead of `torch.compile` (a JIT compiler) might be an issue. For such scenarios, an Ahead-Of-Time (AOT) compilation solution like `torch.export` followed by deployment with a runtime like ONNX Runtime or TensorRT might be more suitable.

### CUDA Graphs Considerations ğŸ“Š
*   Modes like `reduce-overhead` and the Inductor backend (by default) make use of CUDA Graphs.
*   CUDA Graphs work best when the sequence of GPU operations is static. If there's dynamism that prevents graph capture (e.g., tensor shapes changing in ways not handled by dynamic shapes, or CPU operations interspersed with CUDA calls), the benefits of CUDA Graphs diminish.
*   Memory allocated within a CUDA graph capture region has specific lifetime rules, which can sometimes interact with Python's memory management. This is usually handled internally by `torch.compile`.

### Stochasticity (Randomness) ğŸ²
*   Operations involving randomness (e.g., dropout, random number generation) are handled by `torch.compile`.
*   TorchInductor aims to preserve the stochastic behavior. For example, dropout will behave as expected (applied during training, not eval).
*   If you need bit-for-bit reproducibility with random operations across eager and compiled modes, ensure you are properly seeding random number generators. `torch.compile` itself should not alter the sequence of random numbers if an op is compiled. However, fusion and reordering could theoretically change the order in which random ops are called if they were independent, though this is generally handled carefully.
*   `torch._inductor.config.fallback_random = True` can be used to force random operations to fall back to eager, ensuring identical eager behavior, potentially at a performance cost.

### Best Practices Summary ğŸ“
1.  ğŸ‘ **Start simple:** Apply `torch.compile()` with default settings first.
2.  â±ï¸ **Profile:** Use `torch.profiler` to understand bottlenecks in both eager and compiled code.
3.  ğŸ”„ **Iterate on modes/backends:** If `default` isn't optimal, try `reduce-overhead` or other modes/backends based on your model's characteristics.
4.  ğŸ’” **Minimize graph breaks:** Use `TORCH_LOGS` or `explain()` to find and address graph breaks in performance-critical code.
5.  âœ”ï¸ **Test thoroughly:** Verify correctness (numerical precision) and performance across different scenarios.
6.  ğŸ¯ **Target `model.compile()`:** Prefer `model.compile()` over `torch.compile(model)` for `nn.Module` instances if using PyTorch 2.1+ to simplify state_dict management.
7.  ğŸ”¬ **Isolate issues:** When debugging, try to create minimal reproducers and use debugging backends.

---

## 9. Troubleshooting Common Issues ğŸ› ï¸ğŸ†˜
Refer to the official [torch.compile Troubleshooting Guide](https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html) and [FAQ](https://pytorch.org/docs/stable/torch.compiler_faq.html) for the most detailed and up-to-date information. Here's a summary of common points:

### Compilation is Slow ğŸ¢
*   ğŸ¢ **First run:** Expected to be slow due to JIT compilation.
*   ğŸŒ **`max-autotune` mode:** This mode intentionally takes longer to compile to find better kernels.
*   ğŸ˜ **Large models:** Very large or complex models naturally take longer.
*   ğŸ’” **Excessive graph breaks/recompilations:** Many small graph segments can increase total compilation time.
*   ğŸ—„ï¸ **Caching:** `torch.compile` caches compiled artifacts. Ensure caching is working (check `torch._dynamo.config.cache_size_limit`). By default, the cache is in `~/.cache/torch/dynamo` on Linux.

### Code Crashes with `torch.compile` ğŸ’¥ğŸ’»
1.  **Isolate the component:**
    *   Try `backend="eager"`: If it still crashes, the issue is likely in TorchDynamo or your original code interacting unexpectedly with Dynamo's tracing.
    *   Try `backend="aot_eager"`: If this crashes but `eager` worked, the issue is likely in AOTAutograd.
    *   If only `backend="inductor"` (or another specific backend) crashes, the issue is in that backend.
2.  **Minifier:** Use `TORCHDYNAMO_REPRO_AFTER="dynamo"` or `"aot"` to try and get an automatic minimal reproducer.
3.  **Simplify:** Manually simplify your code to create a minimal reproducer.
4.  **Report:** File a GitHub issue with the reproducer and details.

### Incorrect Results (Accuracy Issues) ğŸ“‰â“
*   ğŸ”¢ **Numerical precision:** Compiled code, especially with backends like Inductor using Triton, might use different operation orderings or fused kernels that can lead to minor numerical differences compared to eager. These are usually within acceptable floating-point error margins. Use `torch.allclose` with appropriate tolerances (`atol`, `rtol`) for comparison.
*   **`TORCHDYNAMO_REPRO_LEVEL=4`:** Set this environment variable to help debug accuracy issues. It makes Dynamo try to find the specific operation causing divergence.
*   ğŸŒ¡ï¸ **Mixed precision (AMP):** Ensure AMP (`torch.autocast`) is used consistently if comparing eager AMP with compiled AMP.
*   **Inductor config:** `torch._inductor.config.fallback_random = True` can ensure random ops match eager if that's a source of divergence. `torch.set_float32_matmul_precision('highest')` can sometimes help if matmul precision is an issue.

### Out Of Memory (OOM) Errors ğŸ¤¯
*   ğŸ§  **Compiler memory:** The compilation process itself can consume memory. For very large models, this might be an issue.
*   âš™ï¸ **Backend choices:** Some backends or modes (e.g., `max-autotune`) might use more memory during compilation or generate code that uses more runtime memory.
*   ğŸ“Š **CUDA Graphs:** `torch._inductor.config.triton.cudagraphs = False` can disable CUDA graph usage by Inductor, which might help if OOMs are related to graph capture memory.
*   ã€°ï¸ **Dynamic Shapes:** Try `dynamic=False` if you suspect dynamic shape handling is increasing memory pressure, though this might lead to recompilations.
*   âœ… **Check eager mode:** Ensure your model doesn't OOM in eager mode with the same batch size.

### Excessive Recompilation ğŸ”„
*   â“ **Reason:** Guards are failing. This means assumptions made during compilation (e.g., tensor shapes, strides, values of Python variables) are not holding true for new inputs.
*   ğŸ” **Identify failing guards:** Use `TORCH_LOGS="recompiles,guards"` (or `recompiles_verbose` for more detail). The logs will show which guards failed.
*   ã€°ï¸ **Dynamic Shapes:** If recompiles are due to changing tensor shapes, `dynamic=True` is the primary solution.
*   ğŸ **Changing Python values:** If guards on Python variables (e.g., a learning rate that changes) are causing recompiles, consider if these variables can be made more static or handled outside the compiled region. Wrapping constants in tensors (e.g., `lr=torch.tensor(0.01)`) can sometimes help Dynamo treat them as tensor inputs rather than Python constants to guard on.
*   ğŸ”¢ **Cache limit:** If you hit `torch._dynamo.config.recompile_limit`, Dynamo stops trying to recompile and falls back to eager. You might see a warning. If the number of expected variations is bounded and small, you can increase this limit.

---

## 10. Advanced Options âš™ï¸âœ¨

### `fullgraph=True` ğŸ“ˆğŸ”—
*   `torch.compile(model, fullgraph=True)`
*   If set to `True`, TorchDynamo will attempt to capture the entire model into a single graph.
*   If any graph break is encountered, an error will be raised instead of falling back to eager for parts of the code.
*   **Use Cases:**
    *   Strict performance requirements where any graph break is unacceptable.
    *   Preparing a model for `torch.export`, which typically requires a whole-graph program.
*   Most users do not need `fullgraph=True` for general speedups, as `torch.compile`'s ability to handle graph breaks is a key advantage for usability.

### Backend-specific `options` ğŸ”§
The `torch.compile` function accepts `**kwargs` which are passed as `options` to the chosen backend.
*   These are backend-specific and can control fine-grained compiler behavior.
*   **Example (Inductor):**
    ```python
    # import torch._inductor.config as inductor_config
    # inductor_config.force_fuse_cudagraphs = True # Example, check current available options

    # compiled_model = torch.compile(
    #     model,
    #     options={
    #         "max_autotune": True, # This is also a top-level mode
    #         "epilogue_fusion": False, # Fictional example
    #         # Other inductor-specific flags
    #     }
    # )
    ```
    It's often preferred to set Inductor options via `torch._inductor.config` directly before calling `torch.compile`.
    ```python
    import torch._inductor.config as inductor_config
    inductor_config.triton.cudagraphs = False # Disable Inductor's use of CUDA graphs
    compiled_model = torch.compile(model) # Assuming model is defined
    inductor_config.triton.cudagraphs = True # Reset to default
    ```
*   Consult the documentation for the specific backend (e.g., TorchInductor) to see available options. These are often subject to change.

### Disabling Compilation for Specific Functions (`@torch.compiler.disable`) ğŸš«function
If a specific function within your model is causing issues (e.g., too many graph breaks, crashes, not benefiting from compilation) and you don't want `torch.compile` to attempt to trace or compile it, you can use the `@torch.compiler.disable` decorator.

```python
import torch
from torch import compiler

def helper_function_to_skip(x):
    print("Skipping compilation for this helper")
    # Potentially complex Python logic, I/O, etc.
    return x + torch.randn_like(x) # Still uses PyTorch ops

@compiler.disable
def problematic_part(x):
    print("This part (problematic_part) will run eagerly.")
    # This will also run eagerly by default due to recursive=True
    return helper_function_to_skip(x)

@compiler.disable(recursive=False)
def another_problematic_part(x):
    print("This part (another_problematic_part) will run eagerly.")
    # But if helper_function_to_skip were called from here,
    # Dynamo would attempt to compile it because recursive=False
    return x - 1


@torch.compile
def main_model_logic(x):
    x = x.sin()
    x = problematic_part(x) # This call runs eagerly
    x = x.cos()
    x = another_problematic_part(x) # This call runs eagerly
    return x.relu()

# inp = torch.randn(3, device=device) # Assuming device is defined
# out = main_model_logic(inp)
# print(out)
```
*   When TorchDynamo encounters a call to a `@compiler.disable`-decorated function, it will graph break, execute that function eagerly, and then resume tracing.
*   `recursive=True` (default): Any functions called by the disabled function are also executed eagerly (not compiled).
*   `recursive=False`: Only the decorated function itself is forced to run eagerly. If it calls other PyTorch functions that could be compiled, Dynamo will attempt to trace and compile them.

---

## 11. Conclusion and Further Resources ğŸğŸ“š

`torch.compile` represents a major step forward in making PyTorch both highly usable and highly performant. By understanding its core components, usage patterns, and troubleshooting techniques, you can effectively leverage its power to accelerate your deep learning workflows.

**Key Takeaways:**
*   ğŸ‘ Start with `torch.compile(model)` or `model.compile()` for `nn.Module`s.
*   â³ Be aware of compilation overhead on the first run.
*   ğŸ“Š Use profiling and logging (`TORCH_LOGS`, `torch._dynamo.explain`) to understand performance and identify graph breaks.
*   âš™ï¸ Experiment with modes (`default`, `reduce-overhead`, `max-autotune`) for your specific model and hardware.
*   ã€°ï¸ Use `dynamic=True` if your input tensor shapes vary.
*   ğŸ“¦ For deployment, consider `torch.export` after optimizing with `torch.compile`.

**Further Resources:**
*   **Official PyTorch `torch.compile` Tutorial:** [pytorch.org/tutorials/intermediate/torch_compile_tutorial.html](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
*   **PyTorch 2.0 Get Started Guide:** [pytorch.org/get-started/pytorch-2.0/](https://pytorch.org/get-started/pytorch-2.0/)
*   **`torch.compile` API Documentation:** [pytorch.org/docs/stable/generated/torch.compile.html](https://pytorch.org/docs/stable/generated/torch.compile.html)
*   **Troubleshooting `torch.compile`:** [pytorch.org/docs/stable/torch.compiler_troubleshooting.html](https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html)
*   **`torch.compile` FAQ:** [pytorch.org/docs/stable/torch.compiler_faq.html](https://pytorch.org/docs/stable/torch.compiler_faq.html)
*   **PyTorch Developer Discussions (for deeper dives and updates):** [dev-discuss.pytorch.org](https://dev-discuss.pytorch.org/)

This guide provides a solid foundation. The field of ML compilers is rapidly evolving, so always refer to the latest official PyTorch documentation for the most current information. ğŸ’¡
